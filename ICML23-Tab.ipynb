{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dca9451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataloader import mnist_usps, mnist_reverse, FaceLandmarksDataset, TabDataset\n",
    "from eval import predict, cluster_accuracy, balance, calc_FID\n",
    "from utils import set_seed, AverageMeter, target_distribution, aff, inv_lr_scheduler\n",
    "import argparse\n",
    "\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.datasets import AdultDataset, GermanDataset, BankDataset, CompasDataset, BinaryLabelDataset, CelebADataset, MEPSDataset19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe8d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.ArgumentParser(description='Process some integers.')\n",
    "args.bs = 256\n",
    "args.test_interval = 200\n",
    "args.lr = 1e-3\n",
    "\n",
    "args.num_iter = 1000\n",
    "args.num_sens = 1\n",
    "data_name = 'adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2383f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils import data\n",
    "\n",
    "class TabDataset(data.Dataset):\n",
    "    def __init__(self, dataset, sens_idx):\n",
    "        self.label = dataset.labels.squeeze(-1).astype(int)\n",
    "        \n",
    "        self.feature_size = dataset.features.shape[1]\n",
    "        sens_loc = np.zeros(self.feature_size).astype(bool)\n",
    "        if isinstance(sens_idx, list):\n",
    "            for sens in sens_idx:\n",
    "                sens_loc[sens] = 1\n",
    "        else:\n",
    "            sens_loc[sens_idx] = 1\n",
    "\n",
    "        self.feature = dataset.features[:,~sens_loc] #data without sensitive\n",
    "        self.feature = normalize(self.feature)\n",
    "        \n",
    "        self.sensitive = dataset.features[:,sens_loc]\n",
    "        self.enc = dict()\n",
    "        for i, idx in enumerate(np.unique(self.sensitive, axis = 0)):\n",
    "            self.enc[str(idx)] = i   \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.label[idx]\n",
    "        x = self.feature[idx]\n",
    "        a = self.enc[str(self.sensitive[idx])]\n",
    "        \n",
    "        return x, a, y\n",
    "    \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2b0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = AdultDataset()\n",
    "sens_idx = dataset_orig.feature_names.index('sex')\n",
    "\n",
    "# dataset_orig = CompasDataset()\n",
    "# sens_idx = dataset_orig.feature_names.index('race')\n",
    "\n",
    "data_train, data_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "data_valid, data_test = data_vt.split([0.5], shuffle=True)\n",
    "\n",
    "\n",
    "d_train = TabDataset(data_train, sens_idx)\n",
    "v_train = TabDataset(data_valid, sens_idx)\n",
    "t_train = TabDataset(data_test, sens_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    d_train,\n",
    "    batch_size=args.bs,\n",
    "    shuffle=True,\n",
    "    num_workers = 16\n",
    "    )\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    v_train,\n",
    "    batch_size=args.bs,\n",
    "    shuffle=True,\n",
    "    num_workers = 16\n",
    "    )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    t_train,\n",
    "    batch_size=args.bs,\n",
    "    shuffle=True,\n",
    "    num_workers = 16\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a856b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal(m.weight.data, 1.0, 0.02)\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "        \n",
    "class Encoder_tab(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32):\n",
    "        super(Encoder_tab, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.linear3_1 = nn.Linear(64, latent_dim)\n",
    "        self.linear3_2 = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.children():\n",
    "            weights_init_kaiming(m)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        \n",
    "        mu, logvar = self.linear3_1(x), self.linear3_2(x)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "class Decoder_tab(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Decoder_tab, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 128)\n",
    "        self.linear4 = nn.Linear(128, input_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.children():\n",
    "            weights_init_kaiming(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.relu(self.linear4(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 32, hidden_dim = 128):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.dense1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        for m in self.children():\n",
    "            weights_init_kaiming(m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim = 32, hidden_dim = 128, output_dim = 1):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.dense1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        for m in self.children():\n",
    "            weights_init_kaiming(m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "def permute_dims(z):\n",
    "    assert z.dim() == 2\n",
    "    B, _ = z.size()\n",
    "    perm_z = []\n",
    "    for z_j in z.split(1, 1):\n",
    "        perm = torch.randperm(B).to(z.device)\n",
    "        perm_z_j = z_j[perm]\n",
    "        perm_z.append(perm_z_j)\n",
    "\n",
    "    return torch.cat(perm_z, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224cd42",
   "metadata": {},
   "source": [
    "### CMI Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb256bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch : [0/50]\n",
      "Loss : 130.708\n",
      "recon_loss : 0.314, cls_loss : 1.289, kld_loss : 1.785\n",
      "CI_loss : 0.000, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [1/50]\n",
      "Loss : 118.870\n",
      "recon_loss : 0.290, cls_loss : 1.159, kld_loss : 2.974\n",
      "CI_loss : nan, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [2/50]\n",
      "Loss : 114.381\n",
      "recon_loss : 0.291, cls_loss : 1.112, kld_loss : 3.174\n",
      "CI_loss : 0.001, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [3/50]\n",
      "Loss : 104.916\n",
      "recon_loss : 0.316, cls_loss : 1.009, kld_loss : 4.062\n",
      "CI_loss : 0.005, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [4/50]\n",
      "Loss : 92.870\n",
      "recon_loss : 0.364, cls_loss : 0.878, kld_loss : 5.091\n",
      "CI_loss : 0.017, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [5/50]\n",
      "Loss : 87.241\n",
      "recon_loss : 0.364, cls_loss : 0.822, kld_loss : 5.078\n",
      "CI_loss : 0.024, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [6/50]\n",
      "Loss : 82.836\n",
      "recon_loss : 0.378, cls_loss : 0.777, kld_loss : 5.104\n",
      "CI_loss : 0.025, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [7/50]\n",
      "Loss : 80.585\n",
      "recon_loss : 0.388, cls_loss : 0.755, kld_loss : 5.102\n",
      "CI_loss : 0.026, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [8/50]\n",
      "Loss : 78.983\n",
      "recon_loss : 0.399, cls_loss : 0.739, kld_loss : 5.069\n",
      "CI_loss : 0.027, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [9/50]\n",
      "Loss : 78.009\n",
      "recon_loss : 0.405, cls_loss : 0.729, kld_loss : 5.047\n",
      "CI_loss : 0.028, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [10/50]\n",
      "Loss : 76.902\n",
      "recon_loss : 0.406, cls_loss : 0.718, kld_loss : 5.032\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [11/50]\n",
      "Loss : 76.068\n",
      "recon_loss : 0.407, cls_loss : 0.712, kld_loss : 4.879\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [12/50]\n",
      "Loss : 75.451\n",
      "recon_loss : 0.408, cls_loss : 0.705, kld_loss : 4.887\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [13/50]\n",
      "Loss : 75.081\n",
      "recon_loss : 0.401, cls_loss : 0.702, kld_loss : 4.828\n",
      "CI_loss : 0.030, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [14/50]\n",
      "Loss : 74.759\n",
      "recon_loss : 0.399, cls_loss : 0.699, kld_loss : 4.843\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [15/50]\n",
      "Loss : 74.310\n",
      "recon_loss : 0.408, cls_loss : 0.695, kld_loss : 4.826\n",
      "CI_loss : 0.030, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [16/50]\n",
      "Loss : 74.110\n",
      "recon_loss : 0.404, cls_loss : 0.692, kld_loss : 4.860\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [17/50]\n",
      "Loss : 73.645\n",
      "recon_loss : 0.404, cls_loss : 0.688, kld_loss : 4.833\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [18/50]\n",
      "Loss : 73.741\n",
      "recon_loss : 0.407, cls_loss : 0.689, kld_loss : 4.833\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [19/50]\n",
      "Loss : 73.156\n",
      "recon_loss : 0.404, cls_loss : 0.684, kld_loss : 4.777\n",
      "CI_loss : 0.028, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [20/50]\n",
      "Loss : 73.130\n",
      "recon_loss : 0.401, cls_loss : 0.683, kld_loss : 4.792\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [21/50]\n",
      "Loss : 73.020\n",
      "recon_loss : 0.407, cls_loss : 0.682, kld_loss : 4.797\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [22/50]\n",
      "Loss : 72.694\n",
      "recon_loss : 0.406, cls_loss : 0.679, kld_loss : 4.754\n",
      "CI_loss : 0.029, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [23/50]\n",
      "Loss : 72.725\n",
      "recon_loss : 0.402, cls_loss : 0.680, kld_loss : 4.717\n",
      "CI_loss : 0.028, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [24/50]\n",
      "Loss : 72.469\n",
      "recon_loss : 0.407, cls_loss : 0.678, kld_loss : 4.688\n",
      "CI_loss : 0.028, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [25/50]\n",
      "Loss : 72.527\n",
      "recon_loss : 0.399, cls_loss : 0.678, kld_loss : 4.711\n",
      "CI_loss : 0.028, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [26/50]\n",
      "Loss : 72.659\n",
      "recon_loss : 0.403, cls_loss : 0.679, kld_loss : 4.786\n",
      "CI_loss : 0.028, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [27/50]\n",
      "Loss : 72.499\n",
      "recon_loss : 0.401, cls_loss : 0.678, kld_loss : 4.675\n",
      "CI_loss : 0.027, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [28/50]\n",
      "Loss : 72.633\n",
      "recon_loss : 0.400, cls_loss : 0.680, kld_loss : 4.659\n",
      "CI_loss : 0.027, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [29/50]\n",
      "Loss : 72.257\n",
      "recon_loss : 0.408, cls_loss : 0.675, kld_loss : 4.701\n",
      "CI_loss : 0.027, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [30/50]\n",
      "Loss : 72.246\n",
      "recon_loss : 0.403, cls_loss : 0.675, kld_loss : 4.675\n",
      "CI_loss : 0.026, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [31/50]\n",
      "Loss : 72.175\n",
      "recon_loss : 0.406, cls_loss : 0.675, kld_loss : 4.674\n",
      "CI_loss : 0.026, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [32/50]\n",
      "Loss : 71.764\n",
      "recon_loss : 0.404, cls_loss : 0.671, kld_loss : 4.600\n",
      "CI_loss : 0.026, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [33/50]\n",
      "Loss : 71.799\n",
      "recon_loss : 0.407, cls_loss : 0.672, kld_loss : 4.574\n",
      "CI_loss : 0.026, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [34/50]\n",
      "Loss : 71.672\n",
      "recon_loss : 0.404, cls_loss : 0.671, kld_loss : 4.592\n",
      "CI_loss : 0.025, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [35/50]\n",
      "Loss : 71.690\n",
      "recon_loss : 0.407, cls_loss : 0.671, kld_loss : 4.593\n",
      "CI_loss : 0.025, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [36/50]\n",
      "Loss : 71.731\n",
      "recon_loss : 0.403, cls_loss : 0.671, kld_loss : 4.583\n",
      "CI_loss : 0.025, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [37/50]\n",
      "Loss : 71.622\n",
      "recon_loss : 0.405, cls_loss : 0.671, kld_loss : 4.550\n",
      "CI_loss : 0.025, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [38/50]\n",
      "Loss : 71.510\n",
      "recon_loss : 0.404, cls_loss : 0.670, kld_loss : 4.527\n",
      "CI_loss : 0.024, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [39/50]\n",
      "Loss : 71.463\n",
      "recon_loss : 0.408, cls_loss : 0.669, kld_loss : 4.552\n",
      "CI_loss : 0.023, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [40/50]\n",
      "Loss : 71.399\n",
      "recon_loss : 0.409, cls_loss : 0.669, kld_loss : 4.491\n",
      "CI_loss : 0.023, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [41/50]\n",
      "Loss : 71.303\n",
      "recon_loss : 0.403, cls_loss : 0.667, kld_loss : 4.585\n",
      "CI_loss : 0.023, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [42/50]\n",
      "Loss : 71.133\n",
      "recon_loss : 0.405, cls_loss : 0.666, kld_loss : 4.490\n",
      "CI_loss : 0.022, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [43/50]\n",
      "Loss : 71.351\n",
      "recon_loss : 0.406, cls_loss : 0.669, kld_loss : 4.465\n",
      "CI_loss : 0.022, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [44/50]\n",
      "Loss : 71.186\n",
      "recon_loss : 0.405, cls_loss : 0.667, kld_loss : 4.453\n",
      "CI_loss : 0.022, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [45/50]\n",
      "Loss : 71.164\n",
      "recon_loss : 0.406, cls_loss : 0.667, kld_loss : 4.470\n",
      "CI_loss : 0.021, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [46/50]\n",
      "Loss : 71.024\n",
      "recon_loss : 0.407, cls_loss : 0.666, kld_loss : 4.451\n",
      "CI_loss : 0.021, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [47/50]\n",
      "Loss : 71.010\n",
      "recon_loss : 0.408, cls_loss : 0.665, kld_loss : 4.483\n",
      "CI_loss : 0.020, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [48/50]\n",
      "Loss : 71.025\n",
      "recon_loss : 0.406, cls_loss : 0.666, kld_loss : 4.430\n",
      "CI_loss : 0.020, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [49/50]\n",
      "Loss : 70.710\n",
      "recon_loss : 0.410, cls_loss : 0.663, kld_loss : 4.369\n",
      "CI_loss : 0.020, COV_loss : 0.000\n",
      "\n",
      "[TRAIN] epoch : [50/50]\n",
      "Loss : 70.720\n",
      "recon_loss : 0.411, cls_loss : 0.663, kld_loss : 4.381\n",
      "CI_loss : 0.019, COV_loss : 0.000\n",
      "\n",
      "INPUT :  yr\n",
      "index :  [0, 1]\n",
      "ACC : 0.845, ACC_priv : 0.814, ACC_unpriv : 0.912\n",
      "TPR : 0.585, TPR_priv : 0.602, TPR_unpriv : 0.483\n",
      "FPR : 0.071, FPR_priv : 0.093, FPR_unpriv : 0.035\n",
      "DP : 0.165, EOP : 0.119, EOd : 0.177\n",
      "INPUT :  y\n",
      "index :  [0]\n",
      "ACC : 0.837, ACC_priv : 0.807, ACC_unpriv : 0.901\n",
      "TPR : 0.629, TPR_priv : 0.632, TPR_unpriv : 0.606\n",
      "FPR : 0.096, FPR_priv : 0.116, FPR_unpriv : 0.062\n",
      "DP : 0.153, EOP : 0.026, EOd : 0.080\n",
      "INPUT :  a\n",
      "index :  [2]\n",
      "ACC : 0.756, ACC_priv : 0.693, ACC_unpriv : 0.891\n",
      "TPR : 0.000, TPR_priv : 0.000, TPR_unpriv : 0.000\n",
      "FPR : 0.000, FPR_priv : 0.000, FPR_unpriv : 0.000\n",
      "DP : 0.000, EOP : 0.000, EOd : 0.000\n",
      "INPUT :  r\n",
      "index :  [1]\n",
      "ACC : 0.786, ACC_priv : 0.733, ACC_unpriv : 0.899\n",
      "TPR : 0.219, TPR_priv : 0.216, TPR_unpriv : 0.237\n",
      "FPR : 0.031, FPR_priv : 0.038, FPR_unpriv : 0.020\n",
      "DP : 0.049, EOP : 0.021, EOd : 0.039\n",
      "INPUT :  yra\n",
      "index :  [0, 2, 1]\n",
      "ACC : 0.838, ACC_priv : 0.807, ACC_unpriv : 0.904\n",
      "TPR : 0.601, TPR_priv : 0.627, TPR_unpriv : 0.449\n",
      "FPR : 0.086, FPR_priv : 0.114, FPR_unpriv : 0.041\n",
      "DP : 0.186, EOP : 0.178, EOd : 0.251\n"
     ]
    }
   ],
   "source": [
    "input_dim = dataset_orig.features.shape[1] - 1\n",
    "latent_dim = 6\n",
    "z_dim = [2,2,2]\n",
    "\n",
    "def CI_loss_v2(cls, z_y, z_r, s_batch):\n",
    "    \n",
    "    z_y_repeat = z_y.unsqueeze(1).expand(-1, z_y.shape[0], -1) # N x N (replica) x D\n",
    "    z_r_repeat = z_r.unsqueeze(0).expand(z_y.shape[0], -1, -1) # N (replica) x N  x D\n",
    "\n",
    "    z_yr = torch.cat([z_y_repeat, z_r_repeat], dim = -1).view(z_y.shape[0] **2, -1) # N^2 x 2D\n",
    "    p_y = torch.sigmoid(cls(z_yr).view(z_y.shape[0], z_y.shape[0], -1)) # N x N x 1\n",
    "    p_y_agg = p_y.mean(0)  # mean over different z_y\n",
    "    \n",
    "    H_y_cond_z = -(p_y_agg * torch.log(p_y_agg + 1e-7) + (1-p_y_agg) * torch.log(1-p_y_agg + 1e-7)).mean()\n",
    "    \n",
    "    s_idx = s_batch.view(-1) == 1\n",
    "    p_a = s_idx.float().mean()\n",
    "\n",
    "    p_ya1 = p_y[s_idx].mean(0) * torch.log(p_y[s_idx].mean(0) + 1e-7) + \\\n",
    "                (1-p_y[s_idx].mean(0)) * torch.log(1-p_y[s_idx].mean(0) + 1e-7)\n",
    "    \n",
    "    p_ya0 = p_y[~s_idx].mean(0) * torch.log(p_y[~s_idx].mean(0) + 1e-7) + \\\n",
    "                (1-p_y[~s_idx].mean(0)) * torch.log(1-p_y[~s_idx].mean(0) + 1e-7)\n",
    "    \n",
    "    H_y_cond_za = -(p_a * p_ya1 + (1 - p_a) * p_ya0).mean()\n",
    "    \n",
    "    return H_y_cond_z, H_y_cond_za\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "lambda_ci = 0.8\n",
    "\n",
    "encoder = Encoder_tab(input_dim, latent_dim).cuda()\n",
    "decoder = Decoder_tab(input_dim, latent_dim).cuda()\n",
    "cls_y = Classifier(input_dim = 4, hidden_dim = 16, output_dim =1).cuda()\n",
    "cls_a = Classifier(input_dim = 4, hidden_dim = 16, output_dim =1).cuda()\n",
    "\n",
    "param_lst = list()\n",
    "param_lst += list(encoder.parameters()) + list(decoder.parameters())\n",
    "param_lst += list(cls_y.parameters()) + list(cls_a.parameters())                                       \n",
    "\n",
    "optimizer = torch.optim.Adam(param_lst, lr = 5e-4, weight_decay = 1e-5)\n",
    "\n",
    "H_z_1, H_za = list(), list()\n",
    "H_z_2, H_zs = list(), list()\n",
    "                                               \n",
    "for epoch in (range(epochs + 1)):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    cls_y.train()\n",
    "    cls_a.train()\n",
    "\n",
    "    loss_hist = 0\n",
    "    recon_loss_hist = 0\n",
    "    cls_loss_hist = 0\n",
    "    kld_loss_hist = 0\n",
    "    loss_ci_hist = 0\n",
    "    loss_cov_hist = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    for x_batch, s_batch, y_batch in trainloader:\n",
    "        x_batch = x_batch.cuda().float()\n",
    "        s_batch, y_batch = s_batch.cuda().view(-1,1).float(), y_batch.cuda().view(-1,1).float()\n",
    "        \n",
    "        z, mu, logvar = encoder(x_batch)\n",
    "        \n",
    "        z = z.split(2, dim = -1) # [z_x, z_y, z_r, z_a]\n",
    "        z_y, z_r, z_a = z[0], z[1], z[2]\n",
    "        recon = decoder(torch.cat(z, -1))\n",
    "        \n",
    "        pred_y = cls_y(torch.cat([z_y, z_r], dim =-1))\n",
    "        pred_a = cls_a(torch.cat([z_a, z_r], dim =-1))\n",
    "\n",
    "        recon_loss = F.l1_loss(recon, x_batch)\n",
    "        cls_loss = criterion(pred_y, y_batch) + criterion(pred_a, s_batch)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1))\n",
    "\n",
    "        loss_elbo = 1e2 * cls_loss + kld_loss\n",
    "        \n",
    "        H_y_cond_z_1, H_y_cond_za = CI_loss_v2(cls_y, z_y, z_r, (pred_a>=0).int().detach())\n",
    "        H_y_cond_z_2, H_y_cond_zs = CI_loss_v2(cls_a, z_a, z_r, (pred_y>=0).int().detach())\n",
    "        loss_ci = 0.5 * (H_y_cond_z_1 + H_y_cond_z_2 - H_y_cond_za - H_y_cond_zs)\n",
    "        \n",
    "        H_z_1.append(H_y_cond_z_1.item())\n",
    "        H_za.append(H_y_cond_za.item())\n",
    "        H_z_2.append(H_y_cond_z_2.item())\n",
    "        H_zs.append(H_y_cond_zs.item())\n",
    "        \n",
    "\n",
    "        loss = loss_elbo \n",
    "        if epoch > 5:\n",
    "            loss += lambda_ci * loss_ci\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_hist += loss.item()\n",
    "        recon_loss_hist += recon_loss.item()\n",
    "        cls_loss_hist += cls_loss.item()\n",
    "        kld_loss_hist += kld_loss.item()\n",
    "        loss_ci_hist += loss_ci.item()\n",
    "#         loss_cov_hist += loss_cov.item()\n",
    "        cnt += 1\n",
    "\n",
    "    diag = \"[TRAIN] epoch : [{}/{}]\\n\".format(epoch, epochs)\n",
    "    diag += \"Loss : {:.3f}\\n\".format(loss_hist/cnt)\n",
    "    diag += \"recon_loss : {:.3f}, cls_loss : {:.3f}, kld_loss : {:.3f}\\n\".format(recon_loss_hist/cnt, cls_loss_hist/cnt, kld_loss_hist/cnt)\n",
    "    diag += \"CI_loss : {:.3f}, COV_loss : {:.3f}\\n\".format(loss_ci_hist/cnt, loss_cov_hist/cnt)\n",
    "    print(diag)\n",
    "\n",
    "    \n",
    "    \n",
    "encoder.eval()\n",
    "args.lr = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "for z_input in ['yr', 'y', 'a', 'r', 'yra']:\n",
    "    print(\"INPUT : \", z_input)\n",
    "    z_idx = []\n",
    "    \n",
    "    if 'y' in z_input:\n",
    "        z_idx.append(0)\n",
    "    if 'a' in z_input:\n",
    "        z_idx.append(2)\n",
    "    if 'r' in z_input:\n",
    "        z_idx.append(1)\n",
    "        \n",
    "    print(\"index : \", z_idx)\n",
    "    \n",
    "    cls = Classifier(len(z_input) * 2).cuda()\n",
    "    optimizer = torch.optim.Adam(cls.parameters(), lr = args.lr, weight_decay = 1e-5)\n",
    "\n",
    "    for epoch in (range(epochs + 1)):\n",
    "        cls.train()\n",
    "        for x_batch, s_batch, y_batch in trainloader:\n",
    "            x_batch, s_batch, y_batch = x_batch.cuda().float(), s_batch.cuda().float(), y_batch.cuda().float().view(-1,1)\n",
    "\n",
    "            z, _, _ = encoder(x_batch)\n",
    "            z = z.split(2, dim = 1)\n",
    "            pred = cls(torch.cat([z[idx] for idx in z_idx], dim = -1))\n",
    "            loss = criterion(pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        cls.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_lst, y_lst, a_lst = [], [], []\n",
    "            for x_batch, s_batch, y_batch in validloader:\n",
    "                x_batch, s_batch, y_batch = x_batch.cuda().float(), s_batch.cuda().float().view(-1,1), y_batch.cuda().float().view(-1,1)\n",
    "\n",
    "                z, _, _ = encoder(x_batch)\n",
    "                z = z.split(2, dim = 1)\n",
    "                z_y, z_r, z_a = z[0], z[1], z[2]\n",
    "                pred = cls(torch.cat([z[idx] for idx in z_idx], dim = -1))\n",
    "\n",
    "                pred[pred>=0] = 1\n",
    "                pred[pred<0] = 0\n",
    "\n",
    "                pred_lst.append(pred.detach())\n",
    "                y_lst.append(y_batch.detach())\n",
    "                a_lst.append(s_batch.detach())\n",
    "\n",
    "            pred_lst = torch.cat(pred_lst).cpu().numpy()\n",
    "            y_lst = torch.cat(y_lst).cpu().numpy()\n",
    "            a_lst = torch.cat(a_lst).cpu().numpy()\n",
    "\n",
    "            acc = (pred_lst == y_lst).mean()\n",
    "            acc_priv = (pred_lst[a_lst == 1] == y_lst[a_lst == 1]).mean()\n",
    "            acc_unpriv = (pred_lst[a_lst == 0] == y_lst[a_lst == 0]).mean()\n",
    "            priv_idx = a_lst == 1\n",
    "            pos_idx = y_lst == 1\n",
    "\n",
    "            tpr = (pred_lst[pos_idx] == 1).mean()\n",
    "            tpr_priv = (pred_lst[pos_idx*priv_idx] == 1).mean()\n",
    "            tpr_unpriv = (pred_lst[pos_idx*~priv_idx] == 1).mean()\n",
    "\n",
    "            fpr = (pred_lst[~pos_idx] == 1).mean()\n",
    "            fpr_priv = (pred_lst[~pos_idx*priv_idx] == 1).mean()\n",
    "            fpr_unpriv = (pred_lst[~pos_idx*~priv_idx] == 1).mean()\n",
    "\n",
    "            DP = abs((pred_lst[priv_idx]==1).mean() - (pred_lst[~priv_idx]==1).mean())\n",
    "            EOP =abs(tpr_priv-tpr_unpriv)\n",
    "            EOD =abs(tpr_priv-tpr_unpriv) + abs(fpr_priv-fpr_unpriv)\n",
    "\n",
    "            diag = \"ACC : {:.3f}, ACC_priv : {:.3f}, ACC_unpriv : {:.3f}\".format(acc, acc_priv, acc_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"TPR : {:.3f}, TPR_priv : {:.3f}, TPR_unpriv : {:.3f}\".format(tpr, tpr_priv, tpr_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"FPR : {:.3f}, FPR_priv : {:.3f}, FPR_unpriv : {:.3f}\".format(fpr, fpr_priv, fpr_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"DP : {:.3f}, EOP : {:.3f}, EOd : {:.3f}\".format(DP, EOP, EOD)\n",
    "            print(diag)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca82591",
   "metadata": {},
   "source": [
    "### CI Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d806d912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch : [0/100]\n",
      "Loss : 157.003\n",
      "recon_loss : 0.763, cls_loss : 1.291, kld_loss : 2.402\n",
      "CI_loss : 1.276, \n",
      "\n",
      "[TRAIN] epoch : [1/100]\n",
      "Loss : 147.563\n",
      "recon_loss : 0.767, cls_loss : 1.206, kld_loss : 2.950\n",
      "CI_loss : 1.201, \n",
      "\n",
      "[TRAIN] epoch : [2/100]\n",
      "Loss : 136.528\n",
      "recon_loss : 0.699, cls_loss : 1.120, kld_loss : 3.676\n",
      "CI_loss : 1.041, \n",
      "\n",
      "[TRAIN] epoch : [3/100]\n",
      "Loss : 118.428\n",
      "recon_loss : 0.672, cls_loss : 0.939, kld_loss : 5.676\n",
      "CI_loss : 0.941, \n",
      "\n",
      "[TRAIN] epoch : [4/100]\n",
      "Loss : 111.171\n",
      "recon_loss : 0.687, cls_loss : 0.891, kld_loss : 6.849\n",
      "CI_loss : 0.763, \n",
      "\n",
      "[TRAIN] epoch : [5/100]\n",
      "Loss : 94.987\n",
      "recon_loss : 0.659, cls_loss : 0.744, kld_loss : 6.915\n",
      "CI_loss : 0.684, \n",
      "\n",
      "[TRAIN] epoch : [6/100]\n",
      "Loss : 112.136\n",
      "recon_loss : 0.597, cls_loss : 0.903, kld_loss : 6.572\n",
      "CI_loss : 0.764, \n",
      "\n",
      "[TRAIN] epoch : [7/100]\n",
      "Loss : 97.201\n",
      "recon_loss : 0.587, cls_loss : 0.764, kld_loss : 6.462\n",
      "CI_loss : 0.717, \n",
      "\n",
      "[TRAIN] epoch : [8/100]\n",
      "Loss : 93.904\n",
      "recon_loss : 0.613, cls_loss : 0.736, kld_loss : 6.788\n",
      "CI_loss : 0.675, \n",
      "\n",
      "[TRAIN] epoch : [9/100]\n",
      "Loss : 98.602\n",
      "recon_loss : 0.609, cls_loss : 0.780, kld_loss : 6.634\n",
      "CI_loss : 0.697, \n",
      "\n",
      "[TRAIN] epoch : [10/100]\n",
      "Loss : 90.823\n",
      "recon_loss : 0.638, cls_loss : 0.705, kld_loss : 7.109\n",
      "CI_loss : 0.661, \n",
      "\n",
      "[TRAIN] epoch : [11/100]\n",
      "Loss : 92.309\n",
      "recon_loss : 0.605, cls_loss : 0.718, kld_loss : 6.707\n",
      "CI_loss : 0.690, \n",
      "\n",
      "[TRAIN] epoch : [12/100]\n",
      "Loss : 99.833\n",
      "recon_loss : 0.646, cls_loss : 0.793, kld_loss : 6.801\n",
      "CI_loss : 0.688, \n",
      "\n",
      "[TRAIN] epoch : [13/100]\n",
      "Loss : 85.884\n",
      "recon_loss : 0.596, cls_loss : 0.652, kld_loss : 6.632\n",
      "CI_loss : 0.701, \n",
      "\n",
      "[TRAIN] epoch : [14/100]\n",
      "Loss : 91.459\n",
      "recon_loss : 0.627, cls_loss : 0.713, kld_loss : 6.686\n",
      "CI_loss : 0.672, \n",
      "\n",
      "[TRAIN] epoch : [15/100]\n",
      "Loss : 88.622\n",
      "recon_loss : 0.622, cls_loss : 0.682, kld_loss : 6.495\n",
      "CI_loss : 0.697, \n",
      "\n",
      "[TRAIN] epoch : [16/100]\n",
      "Loss : 98.931\n",
      "recon_loss : 0.607, cls_loss : 0.784, kld_loss : 6.206\n",
      "CI_loss : 0.716, \n",
      "\n",
      "[TRAIN] epoch : [17/100]\n",
      "Loss : 91.617\n",
      "recon_loss : 0.645, cls_loss : 0.712, kld_loss : 6.824\n",
      "CI_loss : 0.678, \n",
      "\n",
      "[TRAIN] epoch : [18/100]\n",
      "Loss : 90.631\n",
      "recon_loss : 0.611, cls_loss : 0.702, kld_loss : 6.415\n",
      "CI_loss : 0.702, \n",
      "\n",
      "[TRAIN] epoch : [19/100]\n",
      "Loss : 90.107\n",
      "recon_loss : 0.620, cls_loss : 0.696, kld_loss : 6.393\n",
      "CI_loss : 0.704, \n",
      "\n",
      "[TRAIN] epoch : [20/100]\n",
      "Loss : 90.043\n",
      "recon_loss : 0.608, cls_loss : 0.700, kld_loss : 6.188\n",
      "CI_loss : 0.690, \n",
      "\n",
      "[TRAIN] epoch : [21/100]\n",
      "Loss : 90.735\n",
      "recon_loss : 0.614, cls_loss : 0.701, kld_loss : 6.286\n",
      "CI_loss : 0.715, \n",
      "\n",
      "[TRAIN] epoch : [22/100]\n",
      "Loss : 92.738\n",
      "recon_loss : 0.609, cls_loss : 0.722, kld_loss : 6.312\n",
      "CI_loss : 0.710, \n",
      "\n",
      "[TRAIN] epoch : [23/100]\n",
      "Loss : 94.478\n",
      "recon_loss : 0.639, cls_loss : 0.754, kld_loss : 6.090\n",
      "CI_loss : 0.650, \n",
      "\n",
      "[TRAIN] epoch : [24/100]\n",
      "Loss : 84.630\n",
      "recon_loss : 0.612, cls_loss : 0.647, kld_loss : 6.029\n",
      "CI_loss : 0.697, \n",
      "\n",
      "[TRAIN] epoch : [25/100]\n",
      "Loss : 91.172\n",
      "recon_loss : 0.599, cls_loss : 0.716, kld_loss : 5.668\n",
      "CI_loss : 0.698, \n",
      "\n",
      "[TRAIN] epoch : [26/100]\n",
      "Loss : 94.527\n",
      "recon_loss : 0.580, cls_loss : 0.747, kld_loss : 5.548\n",
      "CI_loss : 0.715, \n",
      "\n",
      "[TRAIN] epoch : [27/100]\n",
      "Loss : 89.356\n",
      "recon_loss : 0.605, cls_loss : 0.705, kld_loss : 5.498\n",
      "CI_loss : 0.666, \n",
      "\n",
      "[TRAIN] epoch : [28/100]\n",
      "Loss : 94.104\n",
      "recon_loss : 0.599, cls_loss : 0.748, kld_loss : 5.536\n",
      "CI_loss : 0.691, \n",
      "\n",
      "[TRAIN] epoch : [29/100]\n",
      "Loss : 93.444\n",
      "recon_loss : 0.615, cls_loss : 0.742, kld_loss : 5.630\n",
      "CI_loss : 0.680, \n",
      "\n",
      "[TRAIN] epoch : [30/100]\n",
      "Loss : 75.240\n",
      "recon_loss : 0.631, cls_loss : 0.560, kld_loss : 5.595\n",
      "CI_loss : 0.680, \n",
      "\n",
      "[TRAIN] epoch : [31/100]\n",
      "Loss : 88.569\n",
      "recon_loss : 0.634, cls_loss : 0.695, kld_loss : 5.533\n",
      "CI_loss : 0.676, \n",
      "\n",
      "[TRAIN] epoch : [32/100]\n",
      "Loss : 95.946\n",
      "recon_loss : 0.613, cls_loss : 0.771, kld_loss : 5.317\n",
      "CI_loss : 0.675, \n",
      "\n",
      "[TRAIN] epoch : [33/100]\n",
      "Loss : 82.281\n",
      "recon_loss : 0.591, cls_loss : 0.628, kld_loss : 5.396\n",
      "CI_loss : 0.705, \n",
      "\n",
      "[TRAIN] epoch : [34/100]\n",
      "Loss : 84.163\n",
      "recon_loss : 0.626, cls_loss : 0.649, kld_loss : 5.461\n",
      "CI_loss : 0.690, \n",
      "\n",
      "[TRAIN] epoch : [35/100]\n",
      "Loss : 79.981\n",
      "recon_loss : 0.633, cls_loss : 0.609, kld_loss : 5.381\n",
      "CI_loss : 0.683, \n",
      "\n",
      "[TRAIN] epoch : [36/100]\n",
      "Loss : 96.474\n",
      "recon_loss : 0.631, cls_loss : 0.770, kld_loss : 5.432\n",
      "CI_loss : 0.704, \n",
      "\n",
      "[TRAIN] epoch : [37/100]\n",
      "Loss : 82.420\n",
      "recon_loss : 0.612, cls_loss : 0.632, kld_loss : 5.373\n",
      "CI_loss : 0.694, \n",
      "\n",
      "[TRAIN] epoch : [38/100]\n",
      "Loss : 90.703\n",
      "recon_loss : 0.627, cls_loss : 0.710, kld_loss : 5.370\n",
      "CI_loss : 0.714, \n",
      "\n",
      "[TRAIN] epoch : [39/100]\n",
      "Loss : 92.346\n",
      "recon_loss : 0.627, cls_loss : 0.728, kld_loss : 5.521\n",
      "CI_loss : 0.700, \n",
      "\n",
      "[TRAIN] epoch : [40/100]\n",
      "Loss : 82.806\n",
      "recon_loss : 0.610, cls_loss : 0.635, kld_loss : 5.269\n",
      "CI_loss : 0.701, \n",
      "\n",
      "[TRAIN] epoch : [41/100]\n",
      "Loss : 92.874\n",
      "recon_loss : 0.638, cls_loss : 0.739, kld_loss : 5.471\n",
      "CI_loss : 0.676, \n",
      "\n",
      "[TRAIN] epoch : [42/100]\n",
      "Loss : 79.799\n",
      "recon_loss : 0.611, cls_loss : 0.606, kld_loss : 5.275\n",
      "CI_loss : 0.698, \n",
      "\n",
      "[TRAIN] epoch : [43/100]\n",
      "Loss : 87.780\n",
      "recon_loss : 0.600, cls_loss : 0.685, kld_loss : 5.235\n",
      "CI_loss : 0.700, \n",
      "\n",
      "[TRAIN] epoch : [44/100]\n",
      "Loss : 84.859\n",
      "recon_loss : 0.602, cls_loss : 0.655, kld_loss : 5.381\n",
      "CI_loss : 0.699, \n",
      "\n",
      "[TRAIN] epoch : [45/100]\n",
      "Loss : 76.049\n",
      "recon_loss : 0.614, cls_loss : 0.572, kld_loss : 5.241\n",
      "CI_loss : 0.681, \n",
      "\n",
      "[TRAIN] epoch : [46/100]\n",
      "Loss : 82.431\n",
      "recon_loss : 0.626, cls_loss : 0.635, kld_loss : 5.278\n",
      "CI_loss : 0.683, \n",
      "\n",
      "[TRAIN] epoch : [47/100]\n",
      "Loss : 91.153\n",
      "recon_loss : 0.629, cls_loss : 0.720, kld_loss : 5.477\n",
      "CI_loss : 0.682, \n",
      "\n",
      "[TRAIN] epoch : [48/100]\n",
      "Loss : 81.813\n",
      "recon_loss : 0.608, cls_loss : 0.627, kld_loss : 5.363\n",
      "CI_loss : 0.688, \n",
      "\n",
      "[TRAIN] epoch : [49/100]\n",
      "Loss : 81.510\n",
      "recon_loss : 0.578, cls_loss : 0.619, kld_loss : 5.308\n",
      "CI_loss : 0.713, \n",
      "\n",
      "[TRAIN] epoch : [50/100]\n",
      "Loss : 91.102\n",
      "recon_loss : 0.617, cls_loss : 0.724, kld_loss : 5.197\n",
      "CI_loss : 0.676, \n",
      "\n",
      "[TRAIN] epoch : [51/100]\n",
      "Loss : 83.950\n",
      "recon_loss : 0.589, cls_loss : 0.647, kld_loss : 5.213\n",
      "CI_loss : 0.701, \n",
      "\n",
      "[TRAIN] epoch : [52/100]\n",
      "Loss : 90.927\n",
      "recon_loss : 0.623, cls_loss : 0.721, kld_loss : 5.418\n",
      "CI_loss : 0.671, \n",
      "\n",
      "[TRAIN] epoch : [53/100]\n",
      "Loss : 81.101\n",
      "recon_loss : 0.626, cls_loss : 0.618, kld_loss : 5.373\n",
      "CI_loss : 0.698, \n",
      "\n",
      "[TRAIN] epoch : [54/100]\n",
      "Loss : 88.904\n",
      "recon_loss : 0.617, cls_loss : 0.694, kld_loss : 5.380\n",
      "CI_loss : 0.705, \n",
      "\n",
      "[TRAIN] epoch : [55/100]\n",
      "Loss : 82.886\n",
      "recon_loss : 0.572, cls_loss : 0.630, kld_loss : 5.282\n",
      "CI_loss : 0.729, \n",
      "\n",
      "[TRAIN] epoch : [56/100]\n",
      "Loss : 86.018\n",
      "recon_loss : 0.627, cls_loss : 0.669, kld_loss : 5.417\n",
      "CI_loss : 0.686, \n",
      "\n",
      "[TRAIN] epoch : [57/100]\n",
      "Loss : 85.177\n",
      "recon_loss : 0.629, cls_loss : 0.658, kld_loss : 5.491\n",
      "CI_loss : 0.693, \n",
      "\n",
      "[TRAIN] epoch : [58/100]\n",
      "Loss : 86.864\n",
      "recon_loss : 0.608, cls_loss : 0.680, kld_loss : 5.333\n",
      "CI_loss : 0.679, \n",
      "\n",
      "[TRAIN] epoch : [59/100]\n",
      "Loss : 82.915\n",
      "recon_loss : 0.594, cls_loss : 0.631, kld_loss : 5.274\n",
      "CI_loss : 0.726, \n",
      "\n",
      "[TRAIN] epoch : [60/100]\n",
      "Loss : 93.724\n",
      "recon_loss : 0.623, cls_loss : 0.745, kld_loss : 5.333\n",
      "CI_loss : 0.695, \n",
      "\n",
      "[TRAIN] epoch : [61/100]\n",
      "Loss : 93.305\n",
      "recon_loss : 0.624, cls_loss : 0.743, kld_loss : 5.380\n",
      "CI_loss : 0.681, \n",
      "\n",
      "[TRAIN] epoch : [62/100]\n",
      "Loss : 83.958\n",
      "recon_loss : 0.610, cls_loss : 0.647, kld_loss : 5.464\n",
      "CI_loss : 0.689, \n",
      "\n",
      "[TRAIN] epoch : [63/100]\n",
      "Loss : 84.003\n",
      "recon_loss : 0.612, cls_loss : 0.647, kld_loss : 5.229\n",
      "CI_loss : 0.704, \n",
      "\n",
      "[TRAIN] epoch : [64/100]\n",
      "Loss : 87.586\n",
      "recon_loss : 0.607, cls_loss : 0.682, kld_loss : 5.358\n",
      "CI_loss : 0.702, \n",
      "\n",
      "[TRAIN] epoch : [65/100]\n",
      "Loss : 89.443\n",
      "recon_loss : 0.626, cls_loss : 0.701, kld_loss : 5.379\n",
      "CI_loss : 0.696, \n",
      "\n",
      "[TRAIN] epoch : [66/100]\n",
      "Loss : 81.472\n",
      "recon_loss : 0.639, cls_loss : 0.626, kld_loss : 5.394\n",
      "CI_loss : 0.672, \n",
      "\n",
      "[TRAIN] epoch : [67/100]\n",
      "Loss : 79.683\n",
      "recon_loss : 0.585, cls_loss : 0.604, kld_loss : 5.337\n",
      "CI_loss : 0.696, \n",
      "\n",
      "[TRAIN] epoch : [68/100]\n",
      "Loss : 82.672\n",
      "recon_loss : 0.598, cls_loss : 0.635, kld_loss : 5.466\n",
      "CI_loss : 0.687, \n",
      "\n",
      "[TRAIN] epoch : [69/100]\n",
      "Loss : 83.641\n",
      "recon_loss : 0.632, cls_loss : 0.649, kld_loss : 5.416\n",
      "CI_loss : 0.666, \n",
      "\n",
      "[TRAIN] epoch : [70/100]\n",
      "Loss : 88.604\n",
      "recon_loss : 0.621, cls_loss : 0.694, kld_loss : 5.321\n",
      "CI_loss : 0.693, \n",
      "\n",
      "[TRAIN] epoch : [71/100]\n",
      "Loss : 86.183\n",
      "recon_loss : 0.631, cls_loss : 0.671, kld_loss : 5.292\n",
      "CI_loss : 0.691, \n",
      "\n",
      "[TRAIN] epoch : [72/100]\n",
      "Loss : 86.767\n",
      "recon_loss : 0.624, cls_loss : 0.678, kld_loss : 5.192\n",
      "CI_loss : 0.688, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch : [73/100]\n",
      "Loss : 83.402\n",
      "recon_loss : 0.604, cls_loss : 0.643, kld_loss : 5.309\n",
      "CI_loss : 0.688, \n",
      "\n",
      "[TRAIN] epoch : [74/100]\n",
      "Loss : 79.167\n",
      "recon_loss : 0.640, cls_loss : 0.601, kld_loss : 5.361\n",
      "CI_loss : 0.687, \n",
      "\n",
      "[TRAIN] epoch : [75/100]\n",
      "Loss : 82.444\n",
      "recon_loss : 0.615, cls_loss : 0.632, kld_loss : 5.311\n",
      "CI_loss : 0.696, \n",
      "\n",
      "[TRAIN] epoch : [76/100]\n",
      "Loss : 88.425\n",
      "recon_loss : 0.622, cls_loss : 0.691, kld_loss : 5.329\n",
      "CI_loss : 0.698, \n",
      "\n",
      "[TRAIN] epoch : [77/100]\n",
      "Loss : 90.786\n",
      "recon_loss : 0.608, cls_loss : 0.714, kld_loss : 5.394\n",
      "CI_loss : 0.702, \n",
      "\n",
      "[TRAIN] epoch : [78/100]\n",
      "Loss : 79.849\n",
      "recon_loss : 0.604, cls_loss : 0.608, kld_loss : 5.360\n",
      "CI_loss : 0.685, \n",
      "\n",
      "[TRAIN] epoch : [79/100]\n",
      "Loss : 85.796\n",
      "recon_loss : 0.617, cls_loss : 0.668, kld_loss : 5.165\n",
      "CI_loss : 0.693, \n",
      "\n",
      "[TRAIN] epoch : [80/100]\n",
      "Loss : 81.612\n",
      "recon_loss : 0.632, cls_loss : 0.626, kld_loss : 5.313\n",
      "CI_loss : 0.684, \n",
      "\n",
      "[TRAIN] epoch : [81/100]\n",
      "Loss : 76.477\n",
      "recon_loss : 0.607, cls_loss : 0.574, kld_loss : 5.231\n",
      "CI_loss : 0.694, \n",
      "\n",
      "[TRAIN] epoch : [82/100]\n",
      "Loss : 81.491\n",
      "recon_loss : 0.594, cls_loss : 0.620, kld_loss : 5.368\n",
      "CI_loss : 0.704, \n",
      "\n",
      "[TRAIN] epoch : [83/100]\n",
      "Loss : 90.873\n",
      "recon_loss : 0.600, cls_loss : 0.714, kld_loss : 5.327\n",
      "CI_loss : 0.707, \n",
      "\n",
      "[TRAIN] epoch : [84/100]\n",
      "Loss : 82.075\n",
      "recon_loss : 0.605, cls_loss : 0.630, kld_loss : 5.245\n",
      "CI_loss : 0.693, \n",
      "\n",
      "[TRAIN] epoch : [85/100]\n",
      "Loss : 77.284\n",
      "recon_loss : 0.599, cls_loss : 0.583, kld_loss : 4.971\n",
      "CI_loss : 0.703, \n",
      "\n",
      "[TRAIN] epoch : [86/100]\n",
      "Loss : 86.780\n",
      "recon_loss : 0.596, cls_loss : 0.677, kld_loss : 5.182\n",
      "CI_loss : 0.695, \n",
      "\n",
      "[TRAIN] epoch : [87/100]\n",
      "Loss : 81.157\n",
      "recon_loss : 0.584, cls_loss : 0.621, kld_loss : 5.088\n",
      "CI_loss : 0.698, \n",
      "\n",
      "[TRAIN] epoch : [88/100]\n",
      "Loss : 85.383\n",
      "recon_loss : 0.612, cls_loss : 0.664, kld_loss : 5.166\n",
      "CI_loss : 0.692, \n",
      "\n",
      "[TRAIN] epoch : [89/100]\n",
      "Loss : 97.473\n",
      "recon_loss : 0.603, cls_loss : 0.780, kld_loss : 5.266\n",
      "CI_loss : 0.708, \n",
      "\n",
      "[TRAIN] epoch : [90/100]\n",
      "Loss : 92.721\n",
      "recon_loss : 0.626, cls_loss : 0.738, kld_loss : 5.099\n",
      "CI_loss : 0.693, \n",
      "\n",
      "[TRAIN] epoch : [91/100]\n",
      "Loss : 79.790\n",
      "recon_loss : 0.624, cls_loss : 0.610, kld_loss : 5.084\n",
      "CI_loss : 0.687, \n",
      "\n",
      "[TRAIN] epoch : [92/100]\n",
      "Loss : 88.344\n",
      "recon_loss : 0.611, cls_loss : 0.693, kld_loss : 5.038\n",
      "CI_loss : 0.699, \n",
      "\n",
      "[TRAIN] epoch : [93/100]\n",
      "Loss : 86.753\n",
      "recon_loss : 0.612, cls_loss : 0.678, kld_loss : 5.031\n",
      "CI_loss : 0.696, \n",
      "\n",
      "[TRAIN] epoch : [94/100]\n",
      "Loss : 80.352\n",
      "recon_loss : 0.607, cls_loss : 0.614, kld_loss : 5.035\n",
      "CI_loss : 0.696, \n",
      "\n",
      "[TRAIN] epoch : [95/100]\n",
      "Loss : 87.751\n",
      "recon_loss : 0.600, cls_loss : 0.688, kld_loss : 5.132\n",
      "CI_loss : 0.692, \n",
      "\n",
      "[TRAIN] epoch : [96/100]\n",
      "Loss : 80.538\n",
      "recon_loss : 0.620, cls_loss : 0.616, kld_loss : 5.092\n",
      "CI_loss : 0.693, \n",
      "\n",
      "[TRAIN] epoch : [97/100]\n",
      "Loss : 75.526\n",
      "recon_loss : 0.595, cls_loss : 0.564, kld_loss : 5.133\n",
      "CI_loss : 0.701, \n",
      "\n",
      "[TRAIN] epoch : [98/100]\n",
      "Loss : 87.987\n",
      "recon_loss : 0.581, cls_loss : 0.689, kld_loss : 5.165\n",
      "CI_loss : 0.694, \n",
      "\n",
      "[TRAIN] epoch : [99/100]\n",
      "Loss : 83.836\n",
      "recon_loss : 0.602, cls_loss : 0.649, kld_loss : 5.025\n",
      "CI_loss : 0.694, \n",
      "\n",
      "[TRAIN] epoch : [100/100]\n",
      "Loss : 85.613\n",
      "recon_loss : 0.595, cls_loss : 0.667, kld_loss : 5.101\n",
      "CI_loss : 0.693, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = dataset_orig.features.shape[1] - 1\n",
    "z_dim = [2,2,2]\n",
    "latent_dim = sum(z_dim)\n",
    "\n",
    "epochs = 100\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "lambda_fair = 2e1\n",
    "\n",
    "# encoder = [Encoder_tab(input_dim, sub_dim).cuda() for sub_dim in  z_dim]\n",
    "encoder = Encoder_tab(input_dim, latent_dim).cuda()\n",
    "decoder = Decoder_tab(input_dim, latent_dim).cuda()\n",
    "\n",
    "cls_y = Classifier(input_dim = 4, hidden_dim = 16, output_dim =1).cuda()\n",
    "cls_a = Classifier(input_dim = 4, hidden_dim = 16, output_dim =1).cuda()\n",
    "\n",
    "param_lst = list()\n",
    "param_lst += list(encoder.parameters()) + list(decoder.parameters())\n",
    "param_lst += list(cls_y.parameters()) + list(cls_a.parameters())                                       \n",
    "\n",
    "optimizer = torch.optim.Adam(param_lst, lr = 5e-4, weight_decay = 1e-5)\n",
    "                                               \n",
    "dis = Classifier(input_dim = 2, hidden_dim = 16, output_dim =1).cuda()\n",
    "optimizer_dis = torch.optim.Adam(dis.parameters(), lr = 1e-4, weight_decay = 1e-5)\n",
    "    \n",
    "for epoch in (range(epochs + 1)):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    cls_y.train()\n",
    "    cls_a.train()\n",
    "#     [mlp.train() for mlp in encoder]\n",
    "    for x_batch, s_batch, y_batch in trainloader:\n",
    "        x_batch = x_batch.cuda().float()\n",
    "        s_batch, y_batch = s_batch.cuda().view(-1,1).float(), y_batch.cuda().view(-1,1).float()\n",
    "        \n",
    "        z, mu, logvar = encoder(x_batch)\n",
    "        z_y, z_r, z_a = z.split(2, dim = -1)\n",
    "        recon = decoder(z)\n",
    "        pred_y = cls_y(torch.cat([z_y, z_r], dim =-1))\n",
    "        pred_a = cls_a(torch.cat([z_a, z_r], dim =-1))\n",
    "\n",
    "        recon_loss = F.l1_loss(recon, x_batch)\n",
    "        cls_loss = criterion(pred_y, y_batch) + criterion(pred_a, s_batch)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1))\n",
    "\n",
    "        loss_elbo = 1e2 * cls_loss + kld_loss\n",
    "        \n",
    "        ### Agg prob\n",
    "        z_y_repeat = z_y.unsqueeze(1).expand(-1, z_a.shape[0], -1)\n",
    "        z_a_repeat = z_a.unsqueeze(1).expand(-1, z_a.shape[0], -1)\n",
    "        z_r_repeat = z_r.unsqueeze(0).expand(z_a.shape[0], -1, -1)\n",
    "\n",
    "        z_yr = torch.cat([z_y_repeat, z_r_repeat], dim = -1).view(z_a.shape[0] **2, -1)\n",
    "        z_ar = torch.cat([z_a_repeat, z_r_repeat], dim = -1).view(z_a.shape[0] **2, -1)\n",
    "        p_y_agg = cls_y(z_yr).view(z_a.shape[0], z_a.shape[0], -1).mean(0)\n",
    "        p_a_agg = cls_a(z_ar).view(z_a.shape[0], z_a.shape[0], -1).mean(0)\n",
    "        \n",
    "        loss_ci = criterion(dis(torch.cat([p_y_agg, p_a_agg], dim = 1)), torch.zeros_like(y_batch))\n",
    "        \n",
    "        loss = loss_elbo + lambda_fair * (loss_ci)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        z, mu, logvar = encoder(x_batch)\n",
    "        z_y, z_r, z_a = z.split(2, dim = -1)\n",
    "\n",
    "        ### Agg prob\n",
    "        z_y_repeat = z_y.unsqueeze(1).expand(-1, z_a.shape[0], -1)\n",
    "        z_a_repeat = z_a.unsqueeze(1).expand(-1, z_a.shape[0], -1)\n",
    "        z_r_repeat = z_r.unsqueeze(0).expand(z_a.shape[0], -1, -1)\n",
    "\n",
    "        z_yr = torch.cat([z_y_repeat, z_r_repeat], dim = -1).view(z_a.shape[0] **2, -1)\n",
    "        z_ar = torch.cat([z_a_repeat, z_r_repeat], dim = -1).view(z_a.shape[0] **2, -1)\n",
    "        p_y_agg = cls_y(z_yr).view(z_a.shape[0], z_a.shape[0], -1).mean(0)\n",
    "        p_a_agg = cls_a(z_ar).view(z_a.shape[0], z_a.shape[0], -1).mean(0)\n",
    "        \n",
    "        dis_loss = 0.5* (criterion(dis(torch.cat([p_y_agg, p_a_agg], dim = 1)), torch.ones_like(y_batch))\\\n",
    "                    + criterion(dis(torch.cat([p_y_agg, permute_dims(p_a_agg)], dim = 1)), torch.zeros_like(y_batch)))\n",
    "\n",
    "        optimizer_dis.zero_grad()\n",
    "        dis_loss.backward()\n",
    "        optimizer_dis.step()\n",
    "        \n",
    "        \n",
    "    diag = \"[TRAIN] epoch : [{}/{}]\\n\".format(epoch, epochs)\n",
    "    diag += \"Loss : {:.3f}\\n\".format(loss.item())\n",
    "    diag += \"recon_loss : {:.3f}, cls_loss : {:.3f}, kld_loss : {:.3f}\\n\".format(recon_loss.item(), cls_loss.item(), kld_loss.item())\n",
    "    diag += \"CI_loss : {:.3f}, \\n\".format(loss_ci.item())\n",
    "    print(diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd152a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f863ea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT :  yr\n",
      "index :  [0, 1]\n",
      "ACC : 0.849, ACC_priv : 0.817, ACC_unpriv : 0.915\n",
      "TPR : 0.619, TPR_priv : 0.639, TPR_unpriv : 0.506\n",
      "FPR : 0.075, FPR_priv : 0.102, FPR_unpriv : 0.031\n",
      "DP : 0.184, EOP : 0.133, EOd : 0.203\n",
      "INPUT :  y\n",
      "index :  [0]\n",
      "ACC : 0.798, ACC_priv : 0.757, ACC_unpriv : 0.884\n",
      "TPR : 0.303, TPR_priv : 0.311, TPR_unpriv : 0.259\n",
      "FPR : 0.038, FPR_priv : 0.040, FPR_unpriv : 0.035\n",
      "DP : 0.065, EOP : 0.053, EOd : 0.058\n",
      "INPUT :  a\n",
      "index :  [2]\n",
      "ACC : 0.812, ACC_priv : 0.772, ACC_unpriv : 0.896\n",
      "TPR : 0.551, TPR_priv : 0.610, TPR_unpriv : 0.224\n",
      "FPR : 0.102, FPR_priv : 0.154, FPR_unpriv : 0.017\n",
      "DP : 0.256, EOP : 0.386, EOd : 0.524\n",
      "INPUT :  r\n",
      "index :  [1]\n",
      "ACC : 0.848, ACC_priv : 0.813, ACC_unpriv : 0.918\n",
      "TPR : 0.644, TPR_priv : 0.663, TPR_unpriv : 0.537\n",
      "FPR : 0.085, FPR_priv : 0.118, FPR_unpriv : 0.032\n",
      "DP : 0.198, EOP : 0.126, EOd : 0.212\n",
      "INPUT :  yra\n",
      "index :  [0, 2, 1]\n",
      "ACC : 0.848, ACC_priv : 0.816, ACC_unpriv : 0.915\n",
      "TPR : 0.636, TPR_priv : 0.659, TPR_unpriv : 0.510\n",
      "FPR : 0.082, FPR_priv : 0.112, FPR_unpriv : 0.033\n",
      "DP : 0.196, EOP : 0.149, EOd : 0.228\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "# [mlp.eval() for mlp in encoder]\n",
    "\n",
    "args.lr = 1e-3\n",
    "epochs = 20\n",
    "\n",
    "for z_input in ['yr', 'y', 'a', 'r', 'yra']:\n",
    "    print(\"INPUT : \", z_input)\n",
    "    z_idx = []\n",
    "    \n",
    "    if 'y' in z_input:\n",
    "        z_idx.append(0)\n",
    "    if 'a' in z_input:\n",
    "        z_idx.append(2)\n",
    "    if 'r' in z_input:\n",
    "        z_idx.append(1)\n",
    "        \n",
    "    print(\"index : \", z_idx)\n",
    "    \n",
    "    cls = Classifier(len(z_input) * 2).cuda()\n",
    "    optimizer = torch.optim.Adam(cls.parameters(), lr = args.lr, weight_decay = 1e-5)\n",
    "\n",
    "    for epoch in (range(epochs + 1)):\n",
    "        cls.train()\n",
    "        for x_batch, s_batch, y_batch in trainloader:\n",
    "            x_batch, s_batch, y_batch = x_batch.cuda().float(), s_batch.cuda().float(), y_batch.cuda().float().view(-1,1)\n",
    "\n",
    "            z, _, _ = encoder(x_batch)\n",
    "            z = z.split(2, dim = -1)\n",
    "            pred = cls(torch.cat([z[idx] for idx in z_idx], dim = -1))\n",
    "            loss = criterion(pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        cls.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_lst, y_lst, a_lst = [], [], []\n",
    "            for x_batch, s_batch, y_batch in validloader:\n",
    "                x_batch, s_batch, y_batch = x_batch.cuda().float(), s_batch.cuda().float().view(-1,1), y_batch.cuda().float().view(-1,1)\n",
    "\n",
    "                z, _, _ = encoder(x_batch)\n",
    "                z = z.split(2, dim = -1)\n",
    "                z_y, z_r, z_a = z[0], z[1], z[2]\n",
    "                pred = cls(torch.cat([z[idx] for idx in z_idx], dim = -1))\n",
    "\n",
    "                pred[pred>=0] = 1\n",
    "                pred[pred<0] = 0\n",
    "\n",
    "                pred_lst.append(pred.detach())\n",
    "                y_lst.append(y_batch.detach())\n",
    "                a_lst.append(s_batch.detach())\n",
    "\n",
    "            pred_lst = torch.cat(pred_lst).cpu().numpy()\n",
    "            y_lst = torch.cat(y_lst).cpu().numpy()\n",
    "            a_lst = torch.cat(a_lst).cpu().numpy()\n",
    "\n",
    "            acc = (pred_lst == y_lst).mean()\n",
    "            acc_priv = (pred_lst[a_lst == 1] == y_lst[a_lst == 1]).mean()\n",
    "            acc_unpriv = (pred_lst[a_lst == 0] == y_lst[a_lst == 0]).mean()\n",
    "            priv_idx = a_lst == 1\n",
    "            pos_idx = y_lst == 1\n",
    "\n",
    "            tpr = (pred_lst[pos_idx] == 1).mean()\n",
    "            tpr_priv = (pred_lst[pos_idx*priv_idx] == 1).mean()\n",
    "            tpr_unpriv = (pred_lst[pos_idx*~priv_idx] == 1).mean()\n",
    "\n",
    "            fpr = (pred_lst[~pos_idx] == 1).mean()\n",
    "            fpr_priv = (pred_lst[~pos_idx*priv_idx] == 1).mean()\n",
    "            fpr_unpriv = (pred_lst[~pos_idx*~priv_idx] == 1).mean()\n",
    "\n",
    "            DP = abs((pred_lst[priv_idx]==1).mean() - (pred_lst[~priv_idx]==1).mean())\n",
    "            EOP =abs(tpr_priv-tpr_unpriv)\n",
    "            EOD =abs(tpr_priv-tpr_unpriv) + abs(fpr_priv-fpr_unpriv)\n",
    "\n",
    "            diag = \"ACC : {:.3f}, ACC_priv : {:.3f}, ACC_unpriv : {:.3f}\".format(acc, acc_priv, acc_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"TPR : {:.3f}, TPR_priv : {:.3f}, TPR_unpriv : {:.3f}\".format(tpr, tpr_priv, tpr_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"FPR : {:.3f}, FPR_priv : {:.3f}, FPR_unpriv : {:.3f}\".format(fpr, fpr_priv, fpr_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"DP : {:.3f}, EOP : {:.3f}, EOd : {:.3f}\".format(DP, EOP, EOD)\n",
    "            print(diag)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899e992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89bd7492",
   "metadata": {},
   "source": [
    "### COV approximation constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "960f7671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch : [0/50]\n",
      "Loss : 126.927\n",
      "recon_loss : 0.556, cls_loss : 1.263, kld_loss : 0.608\n",
      "CI_loss : 0.001\n",
      "[TRAIN] epoch : [1/50]\n",
      "Loss : 117.475\n",
      "recon_loss : 0.572, cls_loss : 1.154, kld_loss : 2.079\n",
      "CI_loss : 0.002\n",
      "[TRAIN] epoch : [2/50]\n",
      "Loss : 107.933\n",
      "recon_loss : 0.600, cls_loss : 1.048, kld_loss : 3.152\n",
      "CI_loss : 0.007\n",
      "[TRAIN] epoch : [3/50]\n",
      "Loss : 97.850\n",
      "recon_loss : 0.617, cls_loss : 0.938, kld_loss : 4.054\n",
      "CI_loss : 0.015\n",
      "[TRAIN] epoch : [4/50]\n",
      "Loss : 93.313\n",
      "recon_loss : 0.601, cls_loss : 0.893, kld_loss : 3.995\n",
      "CI_loss : 0.020\n",
      "[TRAIN] epoch : [5/50]\n",
      "Loss : 90.768\n",
      "recon_loss : 0.577, cls_loss : 0.870, kld_loss : 3.796\n",
      "CI_loss : 0.022\n",
      "[TRAIN] epoch : [6/50]\n",
      "Loss : 88.510\n",
      "recon_loss : 0.556, cls_loss : 0.846, kld_loss : 3.879\n",
      "CI_loss : 0.022\n",
      "[TRAIN] epoch : [7/50]\n",
      "Loss : 86.661\n",
      "recon_loss : 0.544, cls_loss : 0.828, kld_loss : 3.891\n",
      "CI_loss : 0.022\n",
      "[TRAIN] epoch : [8/50]\n",
      "Loss : 84.562\n",
      "recon_loss : 0.537, cls_loss : 0.805, kld_loss : 4.099\n",
      "CI_loss : 0.022\n",
      "[TRAIN] epoch : [9/50]\n",
      "Loss : 82.681\n",
      "recon_loss : 0.536, cls_loss : 0.784, kld_loss : 4.289\n",
      "CI_loss : 0.021\n",
      "[TRAIN] epoch : [10/50]\n",
      "Loss : 80.910\n",
      "recon_loss : 0.542, cls_loss : 0.764, kld_loss : 4.470\n",
      "CI_loss : 0.021\n",
      "[TRAIN] epoch : [11/50]\n",
      "Loss : 79.285\n",
      "recon_loss : 0.562, cls_loss : 0.747, kld_loss : 4.550\n",
      "CI_loss : 0.020\n",
      "[TRAIN] epoch : [12/50]\n",
      "Loss : 78.726\n",
      "recon_loss : 0.570, cls_loss : 0.741, kld_loss : 4.643\n",
      "CI_loss : 0.020\n",
      "[TRAIN] epoch : [13/50]\n",
      "Loss : 77.733\n",
      "recon_loss : 0.570, cls_loss : 0.731, kld_loss : 4.669\n",
      "CI_loss : 0.020\n",
      "[TRAIN] epoch : [14/50]\n",
      "Loss : 77.304\n",
      "recon_loss : 0.565, cls_loss : 0.726, kld_loss : 4.669\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [15/50]\n",
      "Loss : 76.697\n",
      "recon_loss : 0.569, cls_loss : 0.720, kld_loss : 4.706\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [16/50]\n",
      "Loss : 76.153\n",
      "recon_loss : 0.576, cls_loss : 0.714, kld_loss : 4.745\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [17/50]\n",
      "Loss : 75.705\n",
      "recon_loss : 0.581, cls_loss : 0.711, kld_loss : 4.611\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [18/50]\n",
      "Loss : 75.228\n",
      "recon_loss : 0.583, cls_loss : 0.706, kld_loss : 4.632\n",
      "CI_loss : 0.020\n",
      "[TRAIN] epoch : [19/50]\n",
      "Loss : 74.756\n",
      "recon_loss : 0.572, cls_loss : 0.702, kld_loss : 4.557\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [20/50]\n",
      "Loss : 74.695\n",
      "recon_loss : 0.577, cls_loss : 0.701, kld_loss : 4.600\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [21/50]\n",
      "Loss : 74.411\n",
      "recon_loss : 0.583, cls_loss : 0.698, kld_loss : 4.601\n",
      "CI_loss : 0.020\n",
      "[TRAIN] epoch : [22/50]\n",
      "Loss : 74.048\n",
      "recon_loss : 0.584, cls_loss : 0.695, kld_loss : 4.588\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [23/50]\n",
      "Loss : 73.777\n",
      "recon_loss : 0.584, cls_loss : 0.693, kld_loss : 4.512\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [24/50]\n",
      "Loss : 73.515\n",
      "recon_loss : 0.586, cls_loss : 0.691, kld_loss : 4.448\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [25/50]\n",
      "Loss : 73.264\n",
      "recon_loss : 0.588, cls_loss : 0.688, kld_loss : 4.509\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [26/50]\n",
      "Loss : 73.229\n",
      "recon_loss : 0.583, cls_loss : 0.688, kld_loss : 4.474\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [27/50]\n",
      "Loss : 72.696\n",
      "recon_loss : 0.592, cls_loss : 0.682, kld_loss : 4.479\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [28/50]\n",
      "Loss : 72.773\n",
      "recon_loss : 0.596, cls_loss : 0.683, kld_loss : 4.455\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [29/50]\n",
      "Loss : 72.593\n",
      "recon_loss : 0.591, cls_loss : 0.682, kld_loss : 4.393\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [30/50]\n",
      "Loss : 72.180\n",
      "recon_loss : 0.591, cls_loss : 0.678, kld_loss : 4.393\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [31/50]\n",
      "Loss : 72.170\n",
      "recon_loss : 0.589, cls_loss : 0.678, kld_loss : 4.328\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [32/50]\n",
      "Loss : 71.873\n",
      "recon_loss : 0.599, cls_loss : 0.675, kld_loss : 4.350\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [33/50]\n",
      "Loss : 72.022\n",
      "recon_loss : 0.598, cls_loss : 0.677, kld_loss : 4.338\n",
      "CI_loss : 0.018\n",
      "[TRAIN] epoch : [34/50]\n",
      "Loss : 71.898\n",
      "recon_loss : 0.592, cls_loss : 0.675, kld_loss : 4.381\n",
      "CI_loss : 0.018\n",
      "[TRAIN] epoch : [35/50]\n",
      "Loss : 72.007\n",
      "recon_loss : 0.599, cls_loss : 0.677, kld_loss : 4.345\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [36/50]\n",
      "Loss : 71.652\n",
      "recon_loss : 0.591, cls_loss : 0.673, kld_loss : 4.332\n",
      "CI_loss : 0.018\n",
      "[TRAIN] epoch : [37/50]\n",
      "Loss : 71.461\n",
      "recon_loss : 0.602, cls_loss : 0.671, kld_loss : 4.313\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [38/50]\n",
      "Loss : 71.494\n",
      "recon_loss : 0.595, cls_loss : 0.672, kld_loss : 4.250\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [39/50]\n",
      "Loss : 71.391\n",
      "recon_loss : 0.589, cls_loss : 0.672, kld_loss : 4.232\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [40/50]\n",
      "Loss : 71.255\n",
      "recon_loss : 0.595, cls_loss : 0.670, kld_loss : 4.241\n",
      "CI_loss : 0.018\n",
      "[TRAIN] epoch : [41/50]\n",
      "Loss : 71.321\n",
      "recon_loss : 0.594, cls_loss : 0.670, kld_loss : 4.289\n",
      "CI_loss : 0.018\n",
      "[TRAIN] epoch : [42/50]\n",
      "Loss : 71.051\n",
      "recon_loss : 0.599, cls_loss : 0.668, kld_loss : 4.253\n",
      "CI_loss : 0.018\n",
      "[TRAIN] epoch : [43/50]\n",
      "Loss : 70.998\n",
      "recon_loss : 0.597, cls_loss : 0.667, kld_loss : 4.280\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [44/50]\n",
      "Loss : 70.762\n",
      "recon_loss : 0.592, cls_loss : 0.665, kld_loss : 4.240\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [45/50]\n",
      "Loss : 71.134\n",
      "recon_loss : 0.588, cls_loss : 0.669, kld_loss : 4.240\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [46/50]\n",
      "Loss : 70.634\n",
      "recon_loss : 0.595, cls_loss : 0.664, kld_loss : 4.283\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [47/50]\n",
      "Loss : 70.677\n",
      "recon_loss : 0.596, cls_loss : 0.664, kld_loss : 4.235\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [48/50]\n",
      "Loss : 70.443\n",
      "recon_loss : 0.592, cls_loss : 0.662, kld_loss : 4.232\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [49/50]\n",
      "Loss : 70.488\n",
      "recon_loss : 0.597, cls_loss : 0.663, kld_loss : 4.193\n",
      "CI_loss : 0.019\n",
      "[TRAIN] epoch : [50/50]\n",
      "Loss : 70.273\n",
      "recon_loss : 0.591, cls_loss : 0.660, kld_loss : 4.234\n",
      "CI_loss : 0.019\n",
      "INPUT :  yr\n",
      "index :  [0, 1]\n",
      "ACC : 0.848, ACC_priv : 0.814, ACC_unpriv : 0.919\n",
      "TPR : 0.644, TPR_priv : 0.661, TPR_unpriv : 0.549\n",
      "FPR : 0.084, FPR_priv : 0.116, FPR_unpriv : 0.033\n",
      "DP : 0.194, EOP : 0.112, EOd : 0.195\n",
      "INPUT :  y\n",
      "index :  [0]\n",
      "ACC : 0.797, ACC_priv : 0.748, ACC_unpriv : 0.897\n",
      "TPR : 0.312, TPR_priv : 0.308, TPR_unpriv : 0.333\n",
      "FPR : 0.043, FPR_priv : 0.051, FPR_unpriv : 0.030\n",
      "DP : 0.067, EOP : 0.025, EOd : 0.047\n",
      "INPUT :  a\n",
      "index :  [2]\n",
      "ACC : 0.751, ACC_priv : 0.687, ACC_unpriv : 0.885\n",
      "TPR : 0.000, TPR_priv : 0.000, TPR_unpriv : 0.000\n",
      "FPR : 0.000, FPR_priv : 0.000, FPR_unpriv : 0.000\n",
      "DP : 0.000, EOP : 0.000, EOd : 0.000\n",
      "INPUT :  r\n",
      "index :  [1]\n",
      "ACC : 0.846, ACC_priv : 0.813, ACC_unpriv : 0.916\n",
      "TPR : 0.653, TPR_priv : 0.669, TPR_unpriv : 0.565\n",
      "FPR : 0.090, FPR_priv : 0.122, FPR_unpriv : 0.038\n",
      "DP : 0.194, EOP : 0.104, EOd : 0.188\n",
      "INPUT :  yra\n",
      "index :  [0, 2, 1]\n",
      "ACC : 0.848, ACC_priv : 0.816, ACC_unpriv : 0.914\n",
      "TPR : 0.627, TPR_priv : 0.643, TPR_unpriv : 0.537\n",
      "FPR : 0.078, FPR_priv : 0.104, FPR_unpriv : 0.037\n",
      "DP : 0.179, EOP : 0.106, EOd : 0.174\n"
     ]
    }
   ],
   "source": [
    "# New version\n",
    "\n",
    "input_dim = dataset_orig.features.shape[1] - 1\n",
    "latent_dim = 6\n",
    "z_dim = [2,2,2]\n",
    "\n",
    "# Take cls_ent as both cls_y and cls_a\n",
    "# New version\n",
    "def cov_loss(pred_y, pred_a):\n",
    "    p_y = torch.sigmoid(pred_y)\n",
    "    p_a = torch.sigmoid(pred_a)\n",
    "#     return((p_y * p_a).mean() - p_y.mean() * p_a.mean()) ** 2\n",
    "    return abs((p_y * p_a).mean() - p_y.mean() * p_a.mean())\n",
    "\n",
    "\n",
    "def CI_loss_v2(cls, z_y, z_r, s_batch):\n",
    "    \n",
    "    z_y_repeat = z_y.unsqueeze(1).expand(-1, z_y.shape[0], -1) # N x N (replica) x D\n",
    "    z_r_repeat = z_r.unsqueeze(0).expand(z_y.shape[0], -1, -1) # N (replica) x N  x D\n",
    "\n",
    "    z_yr = torch.cat([z_y_repeat, z_r_repeat], dim = -1).view(z_y.shape[0] **2, -1) # N^2 x 2D\n",
    "    p_y = torch.sigmoid(cls(z_yr).view(z_y.shape[0], z_y.shape[0], -1)) # N x N x 1\n",
    "    p_y_agg = p_y.mean(0)  # mean over different z_y\n",
    "    \n",
    "    H_y_cond_z = -(p_y_agg * torch.log(p_y_agg + 1e-7) + (1-p_y_agg) * torch.log(1-p_y_agg + 1e-7)).mean()\n",
    "    \n",
    "    s_idx = s_batch.view(-1) == 1\n",
    "    p_a = s_idx.float().mean()\n",
    "\n",
    "    p_ya1 = p_y[s_idx].mean(0) * torch.log(p_y[s_idx].mean(0) + 1e-7) + \\\n",
    "                (1-p_y[s_idx].mean(0)) * torch.log(1-p_y[s_idx].mean(0) + 1e-7)\n",
    "    \n",
    "    p_ya0 = p_y[~s_idx].mean(0) * torch.log(p_y[~s_idx].mean(0) + 1e-7) + \\\n",
    "                (1-p_y[~s_idx].mean(0)) * torch.log(1-p_y[~s_idx].mean(0) + 1e-7)\n",
    "    \n",
    "    H_y_cond_za = -(p_a * p_ya1 + (1 - p_a) * p_ya0).mean()\n",
    "    \n",
    "#     return H_y_cond_z - H_y_cond_za\n",
    "    return H_y_cond_z, H_y_cond_za\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "lambda_ci = 0.5\n",
    "\n",
    "encoder = Encoder_tab(input_dim, latent_dim).cuda()\n",
    "decoder = Decoder_tab(input_dim, latent_dim).cuda()\n",
    "cls_y = Classifier(input_dim = 4, hidden_dim = 16, output_dim =1).cuda()\n",
    "cls_a = Classifier(input_dim = 4, hidden_dim = 16, output_dim =1).cuda()\n",
    "\n",
    "param_lst = list()\n",
    "param_lst += list(encoder.parameters()) + list(decoder.parameters())\n",
    "param_lst += list(cls_y.parameters()) + list(cls_a.parameters())                                       \n",
    "\n",
    "optimizer = torch.optim.Adam(param_lst, lr = 5e-4, weight_decay = 1e-4)\n",
    "\n",
    "H_z_1, H_za = list(), list()\n",
    "H_z_2, H_zs = list(), list()\n",
    "                                               \n",
    "for epoch in (range(epochs + 1)):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    cls_y.train()\n",
    "    cls_a.train()\n",
    "\n",
    "    loss_hist = 0\n",
    "    recon_loss_hist = 0\n",
    "    cls_loss_hist = 0\n",
    "    kld_loss_hist = 0\n",
    "    loss_ci_hist = 0\n",
    "    loss_cov_hist = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    for x_batch, s_batch, y_batch in trainloader:\n",
    "        x_batch = x_batch.cuda().float()\n",
    "        s_batch, y_batch = s_batch.cuda().view(-1,1).float(), y_batch.cuda().view(-1,1).float()\n",
    "        \n",
    "        z, mu, logvar = encoder(x_batch)\n",
    "        \n",
    "        z = z.split(2, dim = -1) # [z_x, z_y, z_r, z_a]\n",
    "        z_y, z_r, z_a = z[0], z[1], z[2]\n",
    "        recon = decoder(torch.cat(z, -1))\n",
    "        \n",
    "        pred_y = cls_y(torch.cat([z_y, z_r], dim =-1))\n",
    "        pred_a = cls_a(torch.cat([z_a, z_r], dim =-1))\n",
    "\n",
    "        recon_loss = F.l1_loss(recon, x_batch)\n",
    "        cls_loss = criterion(pred_y, y_batch) + criterion(pred_a, s_batch)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1))\n",
    "\n",
    "#         loss_elbo = recon_loss + 1e2 * cls_loss + kld_loss\n",
    "        loss_elbo = 1e2 * cls_loss + kld_loss\n",
    "        \n",
    "        z_y_repeat = z_y.unsqueeze(1).expand(-1, z_y.shape[0], -1) # N x N (replica) x D\n",
    "        z_a_repeat = z_a.unsqueeze(1).expand(-1, z_y.shape[0], -1) # N x N (replica) x D\n",
    "        z_r_repeat = z_r.unsqueeze(0).expand(z_y.shape[0], -1, -1) # N (replica) x N  x D\n",
    "\n",
    "        # N^2 x 2D\n",
    "        z_yr = torch.cat([z_y_repeat, z_r_repeat], dim = -1).view(z_y.shape[0] **2, -1)\n",
    "        z_ar = torch.cat([z_a_repeat, z_r_repeat], dim = -1).view(z_y.shape[0] **2, -1)\n",
    "\n",
    "        p_y = (cls_y(z_yr).view(z_y.shape[0], z_y.shape[0], -1)) # N x N x 1\n",
    "        p_y_agg = p_y.mean(0)  # mean over different z_y\n",
    "\n",
    "        p_a = (cls_a(z_ar).view(z_y.shape[0], z_y.shape[0], -1)) # N x N x 1\n",
    "        p_a_agg = p_a.mean(0)  # mean over different z_y\n",
    "        loss = loss_elbo \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        \n",
    "        for param in cls_a.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in cls_y.parameters():\n",
    "            param.requires_grad=False\n",
    "            \n",
    "        loss_fair = lambda_ci * cov_loss(pred_y, pred_a)\n",
    "        loss_fair.backward()\n",
    "        \n",
    "        for param in cls_a.parameters():\n",
    "            param.requires_grad=True\n",
    "        for param in cls_y.parameters():\n",
    "            param.requires_grad=True\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_hist += loss.item()\n",
    "        recon_loss_hist += recon_loss.item()\n",
    "        cls_loss_hist += cls_loss.item()\n",
    "        kld_loss_hist += kld_loss.item()\n",
    "        loss_ci_hist += loss_fair.item()\n",
    "#         loss_cov_hist += loss_cov.item()\n",
    "        cnt += 1\n",
    "\n",
    "    diag = \"[TRAIN] epoch : [{}/{}]\\n\".format(epoch, epochs)\n",
    "    diag += \"Loss : {:.3f}\\n\".format(loss_hist/cnt)\n",
    "    diag += \"recon_loss : {:.3f}, cls_loss : {:.3f}, kld_loss : {:.3f}\\n\".format(recon_loss_hist/cnt, cls_loss_hist/cnt, kld_loss_hist/cnt)\n",
    "    diag += \"CI_loss : {:.3f}\".format(loss_ci_hist/cnt)\n",
    "    print(diag)\n",
    "\n",
    "    \n",
    "    \n",
    "encoder.eval()\n",
    "args.lr = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "for z_input in ['yr', 'y', 'a', 'r', 'yra']:\n",
    "    print(\"INPUT : \", z_input)\n",
    "    z_idx = []\n",
    "    \n",
    "    if 'y' in z_input:\n",
    "        z_idx.append(0)\n",
    "    if 'a' in z_input:\n",
    "        z_idx.append(2)\n",
    "    if 'r' in z_input:\n",
    "        z_idx.append(1)\n",
    "        \n",
    "    print(\"index : \", z_idx)\n",
    "    \n",
    "    cls = Classifier(len(z_input) * 2).cuda()\n",
    "    optimizer = torch.optim.Adam(cls.parameters(), lr = args.lr, weight_decay = 1e-5)\n",
    "\n",
    "    for epoch in (range(epochs + 1)):\n",
    "        cls.train()\n",
    "        for x_batch, s_batch, y_batch in trainloader:\n",
    "            x_batch, s_batch, y_batch = x_batch.cuda().float(), s_batch.cuda().float(), y_batch.cuda().float().view(-1,1)\n",
    "\n",
    "            z, _, _ = encoder(x_batch)\n",
    "            z = z.split(2, dim = 1)\n",
    "            pred = cls(torch.cat([z[idx] for idx in z_idx], dim = -1))\n",
    "            loss = criterion(pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        cls.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_lst, y_lst, a_lst = [], [], []\n",
    "            for x_batch, s_batch, y_batch in validloader:\n",
    "                x_batch, s_batch, y_batch = x_batch.cuda().float(), s_batch.cuda().float().view(-1,1), y_batch.cuda().float().view(-1,1)\n",
    "\n",
    "                z, _, _ = encoder(x_batch)\n",
    "                z = z.split(2, dim = 1)\n",
    "                z_y, z_r, z_a = z[0], z[1], z[2]\n",
    "                pred = cls(torch.cat([z[idx] for idx in z_idx], dim = -1))\n",
    "\n",
    "                pred[pred>=0] = 1\n",
    "                pred[pred<0] = 0\n",
    "\n",
    "                pred_lst.append(pred.detach())\n",
    "                y_lst.append(y_batch.detach())\n",
    "                a_lst.append(s_batch.detach())\n",
    "\n",
    "            pred_lst = torch.cat(pred_lst).cpu().numpy()\n",
    "            y_lst = torch.cat(y_lst).cpu().numpy()\n",
    "            a_lst = torch.cat(a_lst).cpu().numpy()\n",
    "\n",
    "            acc = (pred_lst == y_lst).mean()\n",
    "            acc_priv = (pred_lst[a_lst == 1] == y_lst[a_lst == 1]).mean()\n",
    "            acc_unpriv = (pred_lst[a_lst == 0] == y_lst[a_lst == 0]).mean()\n",
    "            priv_idx = a_lst == 1\n",
    "            pos_idx = y_lst == 1\n",
    "\n",
    "            tpr = (pred_lst[pos_idx] == 1).mean()\n",
    "            tpr_priv = (pred_lst[pos_idx*priv_idx] == 1).mean()\n",
    "            tpr_unpriv = (pred_lst[pos_idx*~priv_idx] == 1).mean()\n",
    "\n",
    "            fpr = (pred_lst[~pos_idx] == 1).mean()\n",
    "            fpr_priv = (pred_lst[~pos_idx*priv_idx] == 1).mean()\n",
    "            fpr_unpriv = (pred_lst[~pos_idx*~priv_idx] == 1).mean()\n",
    "\n",
    "            DP = abs((pred_lst[priv_idx]==1).mean() - (pred_lst[~priv_idx]==1).mean())\n",
    "            EOP =abs(tpr_priv-tpr_unpriv)\n",
    "            EOD =abs(tpr_priv-tpr_unpriv) + abs(fpr_priv-fpr_unpriv)\n",
    "\n",
    "            diag = \"ACC : {:.3f}, ACC_priv : {:.3f}, ACC_unpriv : {:.3f}\".format(acc, acc_priv, acc_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"TPR : {:.3f}, TPR_priv : {:.3f}, TPR_unpriv : {:.3f}\".format(tpr, tpr_priv, tpr_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"FPR : {:.3f}, FPR_priv : {:.3f}, FPR_unpriv : {:.3f}\".format(fpr, fpr_priv, fpr_unpriv)\n",
    "            print(diag)\n",
    "\n",
    "            diag = \"DP : {:.3f}, EOP : {:.3f}, EOd : {:.3f}\".format(DP, EOP, EOD)\n",
    "            print(diag)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8404219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4be90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3d651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23c2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1182809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b15237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4e11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9839440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111831cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:main]",
   "language": "python",
   "name": "conda-env-main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
